{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automated data transformation and ingestion from an Amazon S3 bucket to SageMaker Feature Store"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architecture Overview\n",
    "This notebook shows you how to use [AWS Service Catalog](https://aws.amazon.com/servicecatalog), [SageMaker Projects](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-whatis.html), and [Pipelines](https://aws.amazon.com/sagemaker/pipelines/) to create re-usable and portable components in SageMaker Studio.\n",
    "This project automates feature transformations and ingestion into Feature Store, triggered off of new data files that are uploaded to an S3 bucket. The SageMaker project creates all necessary components, sets up all permissions and links between resources.\n",
    "\n",
    "<img src=\"../design/feature-store-ingestion-pipeline.drawio.svg\" style=\"background-color:white;\" alt=\"solution overview\" width=\"1000\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "The following resources must be created before you can proceed with deployment of the SageMaker:\n",
    "- DataWrangler `.flow` file which contains an output node -> done within module [`02-data-wrangler`](../02-data-wrangler/00-data-wrangler-demo.ipynb)\n",
    "- Feature group to store features extracted from the data -> done within the notebook [`01-dw-flow-feature-store`](../02-data-wrangler/01-dw-flow-feature-store.ipynb) in the module `02-data-wrangler`\n",
    "- SageMaker project portfolio -> done with [intial setup](../README.md#deploy-sagemaker-projects)\n",
    "- S3 bucket where new data files will be uploaded"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.session import Session\n",
    "\n",
    "print(sagemaker.__version__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load environment variables from %store\n",
    "%store -r "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%store"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    dw_flow_file_url\n",
    "    dw_output_name\n",
    "    feature_group_name\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] YOU HAVE TO RUN 02-data-wrangler/01-dw-flow-feature-store.ipynb notebook\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set the string literals\n",
    "s3_input_data_prefix = f\"{data_bucket}/feature-store-ingestion-pipeline/landing-zone/\"\n",
    "pipeline_name_prefix = \"s3-fs-ingest-pipeline\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create data load project\n",
    "\n",
    "### Option 1: Create a project in Studio\n",
    "\n",
    "<img src=\"../img/studio-create-project.png\" alt=\"studio-create-project\" width=\"500\"/>\n",
    "\n",
    "#### Select a project template for automated feature ingestion and transformation pipeline\n",
    "\n",
    "<img src=\"../img/studio-select-project-template.png\" alt=\"studio-create-project\" width=\"800\"/>\n",
    "\n",
    "#### Enter your specific project parameters\n",
    "\n",
    "<img src=\"../img/studio-enter-project-parameters.png\" alt=\"studio-create-project\" width=\"800\"/>\n",
    "\n",
    "The parameters are:\n",
    "- Project name and description\n",
    "- **Pipeline name prefix**\n",
    "- **Pipeline description**\n",
    "- **S3 prefix** to monitor to uploaded files to trigger a data transformation and ingestion\n",
    "- **Data Wrangler flow S3 url** with data processing pipeline\n",
    "- **Data Wrangler output name** which generates the feature store input\n",
    "- **Feature group name** to ingest the processed and transformed data\n",
    "- **Lambda execution role**: provide your own IAM role for the lambda function or automatically create a new one\n",
    "\n",
    "Click on **Create project**\n",
    "\n",
    "Wait until project creation is completed. The banner \"Creating project...\":\n",
    "\n",
    "<img src=\"../img/studio-creating-project-banner.png\" alt=\"studio-creating-project-banner\" width=\"500\"/>\n",
    "\n",
    "will change to the project details page:\n",
    "\n",
    "![](img/studio-project-created.png)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set project_id to the project id of the created project\n",
    "project_id = \"p-c98nadneqmvr\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 2: Create project in code\n",
    "Alternatively, you can use [boto3 Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_project) to create a new project from the notebook.  \n",
    "First, get the `ProvisioningArtifactIds` and `ProductId` from service catalog CloudFormation template:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws cloudformation describe-stacks \\\n",
    "    --stack-name \"sm-project-sc-portfolio\" \\\n",
    "    --output table \\\n",
    "    --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cf = boto3.client(\"cloudformation\")\n",
    "\n",
    "r = cf.describe_stacks(StackName=\"sm-project-sc-portfolio\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set parameters for the SageMaker project:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "provisioning_artifact_ids = [v for v in r[\"Stacks\"][0][\"Outputs\"] if v[\"OutputKey\"] == \"ProvisioningArtifactIds\"][0][\"OutputValue\"]\n",
    "product_id = [v for v in r[\"Stacks\"][0][\"Outputs\"] if v[\"OutputKey\"] == \"ProductId\"][0][\"OutputValue\"]\n",
    "project_name = f\"s3-fs-ingest-{strftime('%d-%H-%M-%S', gmtime())}\"\n",
    "project_parameters = [\n",
    "            {\n",
    "                'Key': 'PipelineDescription',\n",
    "                'Value': 'Feature Store ingestion pipeline'\n",
    "            },\n",
    "            {\n",
    "                'Key': 'DataWranglerFlowUrl',\n",
    "                'Value': dw_flow_file_url\n",
    "            },\n",
    "            {\n",
    "                'Key': 'DataWranglerOutputName',\n",
    "                'Value': dw_output_name\n",
    "            },\n",
    "            {\n",
    "                'Key': 'S3DataPrefix',\n",
    "                'Value': s3_input_data_prefix\n",
    "            },\n",
    "            {\n",
    "                'Key': 'FeatureGroupName',\n",
    "                'Value': feature_group_name\n",
    "            },\n",
    "            {\n",
    "                'Key': 'PipelineNamePrefix',\n",
    "                'Value': pipeline_name_prefix\n",
    "            },\n",
    "            \n",
    "        ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, create a SageMaker project from the service catalog product template:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create SageMaker project\n",
    "r = sm.create_project(\n",
    "    ProjectName=project_name,\n",
    "    ProjectDescription=\"Feature Store ingestion from S3\",\n",
    "    ServiceCatalogProvisioningDetails={\n",
    "        'ProductId': product_id,\n",
    "        'ProvisioningArtifactId': provisioning_artifact_ids,\n",
    "        'ProvisioningParameters': project_parameters\n",
    "    },\n",
    ")\n",
    "\n",
    "print(r)\n",
    "project_id = r[\"ProjectId\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with data ingestion project\n",
    "\n",
    "### Project resources\n",
    "The project template creates all necessary resources for an automated data transformation and ingestion:\n",
    "- S3 rule for launching an AWS Lambda function whenever any new data is uploaded to the specified S3 prefix\n",
    "- AWS Lambda function which launches the SageMaker pipeline\n",
    "- SageMaker pipeline which runs a processing job with using a DataWrangler processor\n",
    "- DataWrangler processor which uses a stored `.flow` file with data transformation workflow\n",
    "\n",
    "### CodeCommit repository with seed code\n",
    "All source code for pipeline creation and pipeline parameter configuration is delivered as a CodeCommit repository. The code is fully functional and works out-of-the-box. You own this code and can change any configuration or parameters of the pipeline according to your requirements.\n",
    "\n",
    "To start working with the code you must clone the repository into Studio user's home directory:\n",
    "\n",
    "<img src=\"../img/studo-project-clone-repo.png\" alt=\"studo-project-clone-repo\" width=\"800\"/>\n",
    "\n",
    "You can make your changes to the source code and push it to the CodeCommit repository. The project also delivers an [AWS CodePipeline](https://aws.amazon.com/codepipeline/) CI/CD pipeline which launches an [AWS CodeBuild](https://aws.amazon.com/codebuild/) stage whenever there is a new commit in the repository. The build pulls the code from the repository and calls `create_pipeline` function (file `build.py`). You can change the existing or provide your own code in the `pipeline.create_pipeline` in the file `pipeline.py`. The default code configures a SageMaker pipeline with Data Wrangler processor and upserts the pipeline.\n",
    "\n",
    "### SageMaker pipeline\n",
    "The project delivers a SageMaker pipeline consisting of one processing step with Data Wrangler processor. The pipeline performs the transformation contained in a specified Data Wrangler `.flow` file and stores the transformed features in a specified feature group in the Feature Store.\n",
    "This pipeline is launched by a Lambda function whenever there is a new file uploaded to the specified S3 location. The pipeline is linked to the project and available in the **Pipeline** tab of the project details page:\n",
    "\n",
    "<img src=\"../img/studio-project-details-pipelines.png\" alt=\"studio-project-details-pipelines\" width=\"800\"/>\n",
    "\n",
    "From there you can see the pipeline details, parameters, and the execution history:\n",
    "\n",
    "<img src=\"../img/studio-pipeline-execution-history.png\" alt=\"studio-pipeline-execution-history\" width=\"800\"/>\n",
    "\n",
    "You can also start a new execution manually from Studio:\n",
    "\n",
    "![](img/studio-pipeline-start-execution.png)\n",
    "\n",
    "and provide pipeline parameters:\n",
    "\n",
    "<img src=\"../img/studio-pipeline-parameter-input.png\" alt=\"studio-pipeline-parameter-input\" width=\"500\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the automation pipeline\n",
    "\n",
    "To test the deployed data transformation and feature store ingestion pipeline, perform the following steps:\n",
    "1. Load features from a feature group - via Athena SQL query\n",
    "1. Optionally change the data in the loaded DataFrame\n",
    "1. Export data as `.csv` and save to the monitored S3 location - this will launch the data transformation and ingestion via our pipeline\n",
    "1. Monitor the pipeline execution\n",
    "1. Check the loaded data in the feature group"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm_client = boto3.client(\"sagemaker\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Upload new data to S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a feature group object:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "feature_store_session = Session(\n",
    "    default_bucket=data_bucket\n",
    ")\n",
    "\n",
    "feature_group = FeatureGroup(\n",
    "    name=feature_group_name, \n",
    "    sagemaker_session=feature_store_session\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build SQL query to features group\n",
    "fs_query = feature_group.athena_query()\n",
    "fs_query_output_prefix = \"feature-store-ingestion-pipeline/fs_query_results/\"\n",
    "\n",
    "query_string = f'SELECT * FROM \"{fs_query.table_name}\"'\n",
    "print(f'Prepared query {query_string}')\n",
    "print(fs_query)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run Athena query. The output is loaded to a Pandas dataframe.\n",
    "fs_query.run(\n",
    "    query_string=query_string, \n",
    "    output_location=f\"s3://{data_bucket}/{fs_query_output_prefix}\"\n",
    ")\n",
    "\n",
    "fs_query.wait()\n",
    "data_df = fs_query.as_dataframe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Do some data manipulation - update/insert \n",
    "fs_id = \"39580\"\n",
    "data_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save the data as .csv\n",
    "file_name = f\"new-data-{strftime('%d-%H-%M-%S', gmtime())}.csv\"\n",
    "data_df.to_csv(file_name, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upload the file to S3 prefix. This will launch the Lambda function which will start a new pipeline execution:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Upload data to S3 location. This will launch a new pipeline execution\n",
    "print(f\"Uploading the file {file_name} to s3://{s3_input_data_prefix}\")\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(data_bucket).Object(os.path.join(('/').join(s3_input_data_prefix.split('/')[1:]), file_name)).upload_file(file_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Monitor pipeline execution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "try:\n",
    "    project_id\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Set project_id to the id of the created project \")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set the pipeline name\n",
    "s3_to_fs_pipeline_name = f\"{pipeline_name_prefix}-{project_id}\"\n",
    "\n",
    "%store s3_to_fs_pipeline_name"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check pipeline execution \n",
    "summaries = sm_client.list_pipeline_executions(PipelineName=s3_to_fs_pipeline_name).get('PipelineExecutionSummaries')\n",
    "summaries"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "latest_execution = sm_client.list_pipeline_executions(PipelineName=s3_to_fs_pipeline_name).get('PipelineExecutionSummaries')[0].get('PipelineExecutionArn')\n",
    "print (latest_execution)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Wait for pipeline execution to complete 'Executing' status\n",
    "while sm_client.describe_pipeline_execution(PipelineExecutionArn=latest_execution)[\"PipelineExecutionStatus\"] == \"Executing\":\n",
    "    print('Pipeline is in Executing status...')\n",
    "    time.sleep(60)\n",
    "    \n",
    "print('Pipeline is done Executing')\n",
    "print(sm_client.describe_pipeline_execution(PipelineExecutionArn=latest_execution))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, you can monitor the pipeline execution inside the Pipeline widget of Studio:\n",
    "\n",
    "![](img/studio-pipeline-executing.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the loaded data\n",
    "Once the execution completes, we can check that the data is loaded into the feature group."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Lookup valid Record ID for get_record call\n",
    "fs_id = '39580'\n",
    "\n",
    "query_string = query_string = f'SELECT * FROM \"{fs_query.table_name}\" WHERE fs_id={fs_id}'\n",
    "print(query_string)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fs_query.run(\n",
    "    query_string=query_string, \n",
    "    output_location='s3://'+data_bucket+'/'+fs_query_output_prefix+'/fs_query_results/'\n",
    ")\n",
    "\n",
    "fs_query.wait()\n",
    "data_df = fs_query.as_dataframe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start pipeline run via SDK\n",
    "You can start the data transformation and ingestion pipeline on demand using [SageMaker SDK](https://sagemaker.readthedocs.io/en/v2.57.0/workflows/pipelines/index.html). `pipeline.start` function allows you to provide parameter values to override the default value for the pipeline execution. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get Pipeline object\n",
    "pipeline = Pipeline(name=s3_to_fs_pipeline_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# start execution with the specified parameters\n",
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        InputDataUrl=f\"s3://{s3_input_data_prefix}{file_name}\",\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "execution.wait()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "execution.list_steps()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Change the default values for pipeline parameters\n",
    "To change the default values for the parameters, you can edit `pipeline.py` file with pipeline and parameter definition code:\n",
    "```python\n",
    "    # setup pipeline parameters\n",
    "    p_processing_instance_count = ParameterInteger(\n",
    "        name=\"ProcessingInstanceCount\",\n",
    "        default_value=1\n",
    "    )\n",
    "    p_processing_instance_type = ParameterString(\n",
    "        name=\"ProcessingInstanceType\",\n",
    "        default_value=\"ml.m5.4xlarge\"\n",
    "    )\n",
    "    p_processing_volume_size = ParameterInteger(\n",
    "        name=\"ProcessingVolumeSize\",\n",
    "        default_value=50\n",
    "    )\n",
    "    p_flow_output_name = ParameterString(\n",
    "        name='FlowOutputName',\n",
    "        default_value=flow_output_name\n",
    "    )\n",
    "    p_input_flow = ParameterString(\n",
    "        name='InputFlowUrl',\n",
    "        default_value=data_wrangler_flow_s3_url\n",
    "    )\n",
    "    p_input_data = ParameterString(\n",
    "        name=\"InputDataUrl\",\n",
    "        default_value=input_data_s3_url\n",
    "    )\n",
    "```\n",
    "\n",
    "The pipeline will be automatically started after you commit and push the changes into the project's source code repository."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clean up\n",
    "- delete project\n",
    "- delete s3 buckets\n",
    "- delete CloudFormation stacks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Release resources"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}